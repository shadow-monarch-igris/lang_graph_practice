{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc23d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph ,START,END\n",
    "from typing import TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f3cb7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install google-generativeai==0.5.2 \n",
    "# ! pip install --upgrade langchain-google-genai\n",
    "\n",
    "## for another method to install langchain google genai\n",
    "\n",
    "# ! pip install -U \"langchain[google-genai]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b42e99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user171125/miniconda3/envs/mindenv/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06d7ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######  another way to call gemini model  without langchain ####\n",
    "\n",
    "# import google.generativeai as genai\n",
    "\n",
    "# genai.configure(api_key=\"\")\n",
    "\n",
    "# def call_llm_gemini(full_prompt):\n",
    "#     model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "#     resp = model.generate_content(full_prompt)\n",
    "#     return resp\n",
    "\n",
    "# for testing \n",
    "# wrokflow = call_llm_gemini(\"Hello, my name is John. Can you help me write a short bio about myself?\")\n",
    "# print(wrokflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767589e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key: AIzaSyCrXBtd0j4OxUQQYCx1UlD4t2a9uLhU5Xc\n"
     ]
    }
   ],
   "source": [
    "## call llm with langchain  \n",
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "load_dotenv()\n",
    "# api=os.getenv(\"gen_api\")\n",
    "# print(\"API Key:\", api)  # For debugging purposes only; remove in production\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"gen_api\")\n",
    "\n",
    "model = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\n",
    "\n",
    "# for testing \n",
    "# response = model.invoke(\"Why do parrots talk?\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e308735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph ,START,END\n",
    "from typing import TypedDict\n",
    "# creat states\n",
    "class chatbot_state(TypedDict):\n",
    "    user_input: str\n",
    "    bot_response: str    # to store the response from the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64193d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create node functions\n",
    "def chat_response(state:chatbot_state) -> chatbot_state:\n",
    "    user_input =state[\"user_input\"]\n",
    "    prompt = f\"The user said: {user_input}. Please respond appropriately.\"\n",
    "    response = model.invoke(prompt).content\n",
    "    # response = call_llm_gemini(prompt).text    # for another method to call gemini model without langchain \n",
    "    state[\"bot_response\"] = response\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "608bcbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define graph\n",
    "graph = StateGraph(chatbot_state)\n",
    "\n",
    "# create a graph node\n",
    "graph.add_node(\"chat_response\",chat_response)\n",
    "\n",
    "# add edges to your graph\n",
    "graph.add_edge(START,\"chat_response\")\n",
    "graph.add_edge(\"chat_response\",END)\n",
    "\n",
    "# compile the graph\n",
    "workflow = graph.compile()\n",
    "\n",
    "# execute the graph\n",
    "initial_state:chatbot_state = {\n",
    "    \"user_input\": \"Hello, can you tell me about yourself?\"\n",
    "}   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99230ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.invoke(\"Hello, can you tell me about yourself?\").content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2278fa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm a large language model, trained by Google. I'm here to help you with a wide range of tasks, from answering your questions and providing information to generating creative text formats and translating languages.\n",
      "\n",
      "Think of me as a helpful assistant with a vast amount of knowledge. I can:\n",
      "\n",
      "*   **Answer questions:** If you have a question about anything, I'll do my best to provide a clear and informative answer.\n",
      "*   **Generate text:** I can write stories, poems, code, scripts, musical pieces, email, letters, etc.\n",
      "*   **Summarize information:** If you have a long article or document, I can condense it into a shorter summary.\n",
      "*   **Translate languages:** I can help you understand and communicate in different languages.\n",
      "*   **Brainstorm ideas:** If you're stuck on a creative project, I can help you come up with new ideas.\n",
      "*   **And much more!**\n",
      "\n",
      "I'm constantly learning and improving, so I'm always ready to take on new challenges.\n",
      "\n",
      "**What can I help you with today?**\n"
     ]
    }
   ],
   "source": [
    "final_state = workflow.invoke(initial_state)\n",
    "print(final_state[\"bot_response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34dd7f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
